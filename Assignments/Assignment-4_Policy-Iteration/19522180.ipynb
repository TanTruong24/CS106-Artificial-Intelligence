{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19522180.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYV5xbSZAHe"
      },
      "source": [
        "def policy_improvement(env, V, policy, max_iters=500, gamma=0.9):\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        action_values = np.zeros(env.action_space.n)\n",
        "\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                  action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            # select the best action\n",
        "            best_action = np.argmax(action_values)\n",
        "            policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, game, max_iters=500, gamma=0.9):\n",
        "    start = time.time()\n",
        "    # Start with a random policy\n",
        "    if game == \"frozen-lake\":\n",
        "        rd = random.randint(0,3)\n",
        "    else:\n",
        "        rd = random.randint(0,5)\n",
        "\n",
        "    policy = np.zeros(env.observation_space.n) + rd\n",
        "\n",
        "    # Repeat until convergence or critical number of iterations reached\n",
        "    for i in range(int(max_iters)):\n",
        "\n",
        "        prev_policy = np.copy(policy)\n",
        "\n",
        "        # Policy eveluation\n",
        "        V = policy_evaluation(env, policy, max_iters)\n",
        "\n",
        "        # Policy improvement\n",
        "        policy = policy_improvement(env, V, policy, max_iters)\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(policy, prev_policy)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            print(\"optimal_policy:\", policy)\n",
        "            break\n",
        "\n",
        "    end = time.time()\n",
        "    average_time = (end - start) / i\n",
        "    print(\"Total execution time:\", end - start)\n",
        "    print(\"Average excution time:\", average_time)\n",
        "\n",
        "    return policy, end - start, average_time, i"
      ],
      "metadata": {
        "id": "DGMycU-JgImG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    print(\"Optimal_policy:\", policy)\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    start = time.time()\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    optimal_policy = policy_extraction(env, v_values, gamma=0.9)    \n",
        "\n",
        "    end = time.time()\n",
        "    average_time = (end - start) / i\n",
        "    print(\"Total execution time:\", end - start)\n",
        "    print(\"Average excution time:\", average_time)\n",
        "    \n",
        "    return optimal_policy, end - start, average_time, i"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Hàm đếm số iteration lớn và nhỏ nhất khi hội tụ, thời gian trung bình chạy mỗi thuật toán.\n",
        "def multiple_run_function(env, num_run, fn=\"value_iteration\"):\n",
        "    total_time = []\n",
        "    i_converged = []\n",
        "    for i in range(num_run):\n",
        "        if fn == \"policy_iteration\":\n",
        "            _, total_ex_time, average_ex_time, i_th = policy_iteration(env, \"frozen-lake\", max_iters=500, gamma=0.9)\n",
        "\n",
        "        else:\n",
        "            _, total_ex_time, average_ex_time, i_th = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "        i_converged.append(i_th)\n",
        "        total_time.append(total_ex_time)\n",
        "\n",
        "    i_converged_max, i_converged_min = max(i_converged), min(i_converged)\n",
        "    avg_time_run = sum(total_time) / len(total_time)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f'{fn}: max converged i-th: {i_converged_max}, min converged i-th: {i_converged_min}')\n",
        "    print(f\"Average time {num_run} run: {avg_time_run}\")"
      ],
      "metadata": {
        "id": "JaQ2jowlxmjG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env_FrozenLakeV0 = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "559e0873-db80-49f5-c645-d7eb9fa36067"
      },
      "source": [
        "print(\"observation_space:\", env_FrozenLakeV0.observation_space.n)\n",
        "print(\"env.action_space:\", env_FrozenLakeV0.action_space.n)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 16\n",
            "env.action_space: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_1, total_time_1, average_time_1, _ = policy_iteration(env_FrozenLakeV0, \"frozen-lake\", max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LtfLY4Utiy1",
        "outputId": "d6295e18-d0db-4fd2-de38-06cab8cfe5d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 4-th iteration.\n",
            "optimal_policy: [0. 3. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n",
            "Total execution time: 0.04585003852844238\n",
            "Average excution time: 0.011462509632110596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLakeV0, optimal_policy_1, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPu-5_OmypCs",
        "outputId": "9ffe4e74-e1e1-48ad-ea80-27d8fdcf860b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 743/1000\n",
            "Average number of steps: 36.89636608344549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'policy_iteration'\n",
        "multiple_run_function(env_FrozenLakeV0, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWU2UOw2AA3u",
        "outputId": "69d05e01-7846-4a18-bd4a-775f16fc74ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_iteration: max converged i-th: 5, min converged i-th: 1\n",
            "Average time 500 run: 0.038978415966033936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_2, total_time_2, average_time_2, _ = value_iteration(env_FrozenLakeV0, max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI1yd1-xkh1H",
        "outputId": "af42c074-2778-4c3a-ee9c-033edfacee65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n",
            "Optimal_policy: [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "Total execution time: 0.052996158599853516\n",
            "Average excution time: 0.0006708374506310572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLakeV0, optimal_policy_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWfXMRnSkyXY",
        "outputId": "568514f7-d11c-41ca-d35b-6d79b604a77f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 738/1000\n",
            "Average number of steps: 37.31436314363144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'value_iteration'\n",
        "multiple_run_function(env_FrozenLakeV0, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY1eXPbF7-d3",
        "outputId": "49595fd6-ad8e-4fa7-893a-c12d2bdf6a7f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value_iteration: max converged i-th: 79, min converged i-th: 79\n",
            "Average time 500 run: 0.050828081130981445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Đối với toy game FrozenLake-v0\n",
        "# 1. Policy iteration\n",
        "#     policy iteration cho kết quả hội tụ rất nhanh từ 1 đến 5 iteration. \n",
        "#     Thời gian trung bình cho mỗi lần thực thi policy iteration cho kết quả là 0.03748822069168091 giây\n",
        "# 2. Value iteration\n",
        "#     Value iteration cho kết quả hội tụ lâu hơn policy iteration, phải mất 79 iteration để value iteration hội tụ ra kết quả (so với 1-5 iteration)\n",
        "#     Thời gian trung bình cho mỗi lần thực thi value iteration cho kết quả là 0.054465441226959226 giây, giá trị này lớn hơn thời gian của policy iteration.\n",
        "#     Tuy nhiên, nếu so sánh thời gian thực thi ở mỗi iteration thì value iteration nhanh hơn policy iteration (0.00064 < 0.0215 s)\n",
        "#\n",
        "# Số lần thành công khi thực thi nhiều lần hàm play_multiple_times() của policy iteration lớn hơn, trung bình bước nhỏ hơn của value iteration.\n",
        "# -> với toy game FrozenLake-v0 thuật toán Policy iteration cho kết quả tốt hơn\n"
      ],
      "metadata": {
        "id": "1KG2T8nrwea6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_FrozenLake8x8V0 = gym.make('FrozenLake8x8-v0')"
      ],
      "metadata": {
        "id": "JcWnxf_thoB2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"observation_space:\", env_FrozenLake8x8V0.observation_space.n)\n",
        "print(\"env.action_space:\", env_FrozenLake8x8V0.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elqUj7z_iOXb",
        "outputId": "aeca8794-d4ac-497f-e9ad-aa583dbbdc31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 64\n",
            "env.action_space: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_1, total_time_1, average_time_1, _ = policy_iteration(env_FrozenLake8x8V0, \"frozen-lake\", max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz0SX7BHiWRK",
        "outputId": "21ecc935-8898-4e00-bf6f-d4c268d24d73"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 9-th iteration.\n",
            "optimal_policy: [3. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 1. 3. 3. 0. 0. 2. 3. 2. 1.\n",
            " 3. 3. 3. 1. 0. 0. 2. 1. 3. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
            " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            "Total execution time: 0.21987295150756836\n",
            "Average excution time: 0.024430327945285372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLake8x8V0, optimal_policy_1, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZqJyIcwiael",
        "outputId": "10ff8e92-8bc8-4210-e48e-e3ea4e6728e5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 730/1000\n",
            "Average number of steps: 70.23013698630137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'policy_iteration'\n",
        "multiple_run_function(env_FrozenLake8x8V0, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2074808-5277-425e-fbe5-5f406b00b8c2",
        "id": "bLCwwcgoGmZ-"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_iteration: max converged i-th: 9, min converged i-th: 2\n",
            "Average time 500 run: 0.16743802547454834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_2, total_time_2, average_time_2, _ = value_iteration(env_FrozenLake8x8V0, max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30f34b8-811c-4cbf-9a2b-c21b6ecf4544",
        "id": "99alETp3nEix"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 117-th iteration.\n",
            "Optimal_policy: [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
            "Total execution time: 0.17261433601379395\n",
            "Average excution time: 0.001475336205246102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLake8x8V0, optimal_policy_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07849f23-0fec-498f-8e9d-858c44bacbe9",
        "id": "edAWmfMrnJ9D"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 751/1000\n",
            "Average number of steps: 73.5392809587217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'value_iteration'\n",
        "multiple_run_function(env_FrozenLake8x8V0, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e362d326-7d5c-41ed-e787-69d0eb6cee8e",
        "id": "9ykkESFZGZoM"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value_iteration: max converged i-th: 117, min converged i-th: 117\n",
            "Average time 500 run: 0.21420513343811035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Đối với toy game FrozenLake8x8-v0\n",
        "# 1. Policy iteration\n",
        "#     policy iteration cho kết quả hội tụ rất nhanh từ 2 đến 9 iteration. \n",
        "#     Thời gian trung bình cho mỗi lần thực thi policy iteration cho kết quả là 0.16743802547454834 giây\n",
        "# 2. Value iteration\n",
        "#     Value iteration cho kết quả hội tụ lâu hơn policy iteration, phải mất 117 iteration để value iteration hội tụ ra kết quả (so với 2-9 iteration, gấp hơn 13 lần)\n",
        "#     Thời gian trung bình cho mỗi lần thực thi value iteration cho kết quả là 0.21420513343811035 giây, giá trị này lớn hơn thời gian của policy iteration.\n",
        "#     Tuy nhiên, nếu so sánh thời gian thực thi ở mỗi iteration thì value iteration nhanh hơn policy iteration (0.0022 < 0.044 s)\n",
        "#\n",
        "# Số lần thành công và trung bình bước khi thực thi nhiều lần hàm play_multiple_times() của policy iteration và value iteration chênh lệch nhau tùy lần chạy, khó xác định thuật toán nào hơn.\n",
        "# -> với toy game FrozenLake8x8-v0, nếu dựa vào thời gian thực thi thì thuật toán policy iteration cho kết quả tốt hơn.\n"
      ],
      "metadata": {
        "id": "A81tYY11Gquc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_TaxiV3 = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "ow2AALVknNoQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"observation_space:\", env_TaxiV3.observation_space.n)\n",
        "print(\"env.action_space:\", env_TaxiV3.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562cd557-082f-4d3d-e790-957efb43422d",
        "id": "pGjzQdPanTLa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 500\n",
            "env.action_space: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_1, total_time_1, average_time_1, _ = policy_iteration(env_TaxiV3, \"frozen-lake\", max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9f3535-b61a-4b64-d996-a9fb31efeffc",
        "id": "zjpS5a-Mnebb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 16-th iteration.\n",
            "optimal_policy: [4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 3. 3. 3. 3.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 5. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 2.\n",
            " 1. 2. 0. 2. 1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 2. 1. 2. 3. 2.\n",
            " 3. 3. 3. 3. 1. 1. 1. 1. 3. 3. 3. 3. 2. 2. 2. 2. 3. 1. 3. 2. 3. 3. 3. 3.\n",
            " 1. 1. 1. 1. 3. 3. 3. 3. 0. 0. 0. 0. 3. 1. 3. 0. 3. 3. 3. 3. 1. 1. 1. 1.\n",
            " 3. 3. 3. 3. 0. 0. 0. 0. 3. 1. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 4. 4. 4. 4. 1. 1. 1. 1. 1. 1. 5. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 4. 4. 4. 1. 1. 1. 5.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 3. 3. 3. 1. 1. 1. 3.]\n",
            "Total execution time: 2.6627254486083984\n",
            "Average excution time: 0.1664203405380249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_TaxiV3, optimal_policy_1, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1f5488-1e23-404c-c3a2-f539916ede83",
        "id": "B6cm2gMWnhia"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'policy_iteration'\n",
        "multiple_run_function(env_TaxiV3, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56259f3c-a27d-4e65-ddd2-c24a7d795f4a",
        "id": "0N8b_xIJGohz"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_iteration: max converged i-th: 16, min converged i-th: 16\n",
            "Average time 500 run: 2.6518786878585816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_2, total_time_2, average_time_2, _= value_iteration(env_TaxiV3, max_iters=500, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180ba90e-65d6-483e-ebaa-c31d0b9225d4",
        "id": "_-1NMcXInGko"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 116-th iteration.\n",
            "Optimal_policy: [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n",
            "Total execution time: 1.267113208770752\n",
            "Average excution time: 0.010923389730782345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_TaxiV3, optimal_policy_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d070b56-12c5-42fa-e65e-a6ef75f1c9c6",
        "id": "x3r3E0zmnKNE"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 12.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 500\n",
        "fn = 'value_iteration'\n",
        "multiple_run_function(env_TaxiV3, num_run, fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca0e5f2-028d-4872-a36a-2cf68c669cf2",
        "id": "CyPHHdHhGb0E"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value_iteration: max converged i-th: 116, min converged i-th: 116\n",
            "Average time 500 run: 1.3672375087738038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Đối với toy game Taxi-v3\n",
        "# 1. Policy iteration\n",
        "#     policy iteration cho kết quả hội tụ rất nhanh 16 iteration (trong cả 500 lần chạy). \n",
        "#     Thời gian trung bình cho mỗi lần thực thi policy iteration cho kết quả là 2.6518786878585816 giây\n",
        "# 2. Value iteration\n",
        "#     Value iteration cho kết quả hội tụ lâu hơn policy iteration, phải mất 116 iteration để value iteration hội tụ ra kết quả (so với 16 iteration)\n",
        "#     Thời gian trung bình cho mỗi lần thực thi value iteration cho kết quả là 1.3672375087738038 giây, giá trị này nhỏ hơn thời gian của policy iteration.\n",
        "#\n",
        "# Số lần thành công khi thực thi hàm play_multiple_times() của policy iteration và value iteration bằng nhau, số trung bình bước của value iteration nhỏ hơn.\n",
        "# -> với toy game Taxi-v0, tuy số iteration để hội tụ nhiều hơn nhưng thời gian thực thi ngắn hơn, số bước trung bình nhỏ hơn, và số lần thành công như nhau\n",
        "#     thì thuật toán value iteration tốt hơn trong trường hợp game này\n"
      ],
      "metadata": {
        "id": "ej9jgWHVGrpo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhìn chung đối với game ít trạng thái thì thuật toán policy iteration cho hiệu quả hơn (thời gian hội tụ nhanh) so với value iteration. Tuy nhiên khi số lượng\n",
        "# trạng thái nhiều hơn thì value iteration lại hiệu quả hơn, số iteration để hội tụ có thể nhiều hơn policy iteration nhưng thời gian thực thi ngắn hơn và\n",
        "# kết quả khi chơi thành công không chênh lệch so với policy iteration."
      ],
      "metadata": {
        "id": "secrFavnThTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}