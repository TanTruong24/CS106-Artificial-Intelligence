{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19522180-Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chào cả lớp,\n",
        "\n",
        "Dựa vào Q-Learning trong file đính kèm, các em hãy cày đặt thuật toán SARSA nhé (slides Reinforcement Learning).\n",
        "\n",
        "Các em có nhận xét so sánh gì về performance của Q-Learning và SARSA trên 3 env \"FrozenLake-v0\", \"FrozenLake8x8-v0\", và \"Taxi-v3\"? Viết nhận xét trực tiếp vào file bài nộp ipynb. Đặt tên file MSSV.ipynb với MSSV của mình.\n",
        "\n",
        "Deadline: 05/06/2022\n",
        "\n",
        "Sau ngày 12/06/2022 sẽ không nhân thêm bài nộp mới."
      ],
      "metadata": {
        "id": "KoLp9JgTTCVc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8P_laMcSQNk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGopsD0IWpDO"
      },
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, q_table, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, q_table)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "metadata": {
        "id": "2l8BKi9TSqRe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFsyfXH5Ssd6"
      },
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3xVez-WTeww"
      },
      "source": [
        "def q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    start = time.time()\n",
        "\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        for step in range(num_steps_per_episode):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(f'Episode {episode} finished')\n",
        "    print(\"Total execution time:\", end - start)\n",
        "    print(\"Average execution time:\", (end - start) / episode)\n",
        "    return q_table, rewards_all, (end - start) / episode, end - start"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to choose the next action\n",
        "def choose_action(env, state, epsilon, Q):\n",
        "    action=0\n",
        "    exploration = random.uniform(0,1)\n",
        "    if exploration < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state, :])\n",
        "\n",
        "    return action\n",
        "\n",
        "def SARSA(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    start = time.time()   \n",
        "  \n",
        "    #Initializing the Q-matrix\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "\n",
        "    # Starting the SARSA learning\n",
        "    for episode in range(num_episodes):\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "\n",
        "        state1 = env.reset()\n",
        "        action1 = choose_action(env, state1, epsilon, q_table)\n",
        "\n",
        "        #Initializing the reward\n",
        "        reward_episode = 0\n",
        "\n",
        "        for step in range(num_steps_per_episode):\n",
        "            \n",
        "            #Getting the next state\n",
        "            next_state, reward, done, info = env.step(action1)\n",
        "\n",
        "            #Choosing the next action\n",
        "            action2 = choose_action(env, next_state, epsilon, q_table)\n",
        "            \n",
        "            #Learning the Q-value\n",
        "            predict = q_table[state1, action1]\n",
        "            target = reward + gamma * q_table[next_state, action2]\n",
        "            q_table[state1, action1] = q_table[state1, action1] + learning_rate * (target - predict)\n",
        "\n",
        "            state1 = next_state\n",
        "            action1 = action2\n",
        "            \n",
        "            #Updating the respective vaLues\n",
        "            reward_episode += reward\n",
        "            \n",
        "            #If at the end of learning process\n",
        "            if done:\n",
        "              break\n",
        "\n",
        "        rewards_all.append(reward_episode)\n",
        "\n",
        "    end = time.time()   \n",
        "\n",
        "    print(f'Episode {episode} finished')\n",
        "    print(\"Total execution time:\", end - start)\n",
        "    print(\"Average execution time:\", (end - start) / episode)\n",
        "    return q_table, rewards_all, (end - start) / episode, end - start"
      ],
      "metadata": {
        "id": "mUiEa73C26Jr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifGZ8j-SWPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d4070b-c836-4e4a-f0d5-73c90e604b84"
      },
      "source": [
        "env_FrozenLakeV0 = gym.make('FrozenLake-v0')\n",
        "\n",
        "print(\"observation_space:\", env_FrozenLakeV0.observation_space.n)\n",
        "print(\"env.action_space:\", env_FrozenLakeV0.action_space.n)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 16\n",
            "env.action_space: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, rewards_all, avg_time, total_time = SARSA(env_FrozenLakeV0, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfMw1Z2z5Ffp",
        "outputId": "8d521d4c-807e-42df-d914-4933caaa1ecc"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 12.030235767364502\n",
            "Average execution time: 0.0006015418654614982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJhpd_Ie_Gum",
        "outputId": "ef2de531-0ee7-4edb-9444-1b6ebf4242ff"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12647.0"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLakeV0, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF4svvqD_-A9",
        "outputId": "68849071-93d9-4401-d861-55ac5210dbc4"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 733/1000\n",
            "Average number of steps: 35.8431105047749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmauQUIQVOWr",
        "outputId": "e7eec05b-18a5-4fd6-9e3f-c5dcf4e5982c"
      },
      "source": [
        "q_table, rewards_all, avg_time, total_time = q_learning(env_FrozenLakeV0, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 21.11684012413025\n",
            "Average execution time: 0.0010558948009465599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f15d299-cf5d-41b1-e1f6-4f0aa2808ef7",
        "id": "Y_TWl1t5IGbn"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13323.0"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLakeV0, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs-EbCUUSvf2",
        "outputId": "fddf9b2f-56c1-4dae-dda5-dd5e8336ef07"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 738/1000\n",
            "Average number of steps: 38.70867208672087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Với toy game FrozenLake-v0, thuật toán SARSA cho thời gian thực thi nhanh hơn và số bước trung bình nhỏ hơn nhỏ hơn Q-Learning.\n",
        "# Số lần chơi thành công trong 1000 lần chơi của mỗi thuật toán tương đương nhau, tùy lần thực hiện.\n",
        "# thuật toán có thể không ra kết quả ở một số lần chạy.\n",
        "# Nhìn chung thuật toán SARSA cho hiệu quả tốt hơn ở toy game FrozenLake-v0"
      ],
      "metadata": {
        "id": "bm4CcsAzSx-f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cba73b4-29c7-4b48-f1f7-e44ab0155a0a",
        "id": "7sSreSXOtjFE"
      },
      "source": [
        "env_FrozenLake8x8V0 = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "print(\"observation_space:\", env_FrozenLake8x8V0.observation_space.n)\n",
        "print(\"env.action_space:\", env_FrozenLake8x8V0.action_space.n)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 64\n",
            "env.action_space: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, rewards_all, avg_time, total_time = SARSA(env_FrozenLake8x8V0, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6987e9aa-e5cd-4231-9cf5-2b6c659632e1",
        "id": "HwshMGCHtjFG"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 29.495771646499634\n",
            "Average execution time: 0.0014748623254412538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36609488-7a37-4344-a82a-85d6e9b63f97",
        "id": "7szsmSpPIH-c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLake8x8V0, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1a62e6-98f6-4eae-dc25-1393460e22a2",
        "id": "cZPWqXlPtjFI"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954d3ecd-b518-4b79-e5d3-54194334146e",
        "id": "dnkY6teltjFJ"
      },
      "source": [
        "q_table, rewards_all, avg_time, total_time = q_learning(env_FrozenLake8x8V0, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 50.60630130767822\n",
            "Average execution time: 0.002530441587463284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d206bf38-0247-4d80-fd28-c1ffe9b38a0f",
        "id": "IgVKZY9ZIIhm"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_FrozenLake8x8V0, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1487082-6fdb-4772-d8fc-e49cd6cbbc99",
        "id": "fStYFP0XtjFL"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# với toy game FrozenLake8x8-v0, cả 2 thuật toán Q-Learning và SARSA đều không cho kết quả, dù đã thực hiện chạy nhiều lần."
      ],
      "metadata": {
        "id": "KXwC2vl8tjFM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536b9692-3d67-4d24-e27e-e5335cae5541",
        "id": "UGBZpIxDtk2R"
      },
      "source": [
        "env_TaxiV3 = gym.make('Taxi-v3')\n",
        "\n",
        "print(\"observation_space:\", env_TaxiV3.observation_space.n)\n",
        "print(\"env.action_space:\", env_TaxiV3.action_space.n)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 500\n",
            "env.action_space: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, rewards_all, avg_time, total_time = SARSA(env_TaxiV3, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6fddbf-6bcf-4a13-f312-5b5ab91b67ba",
        "id": "qdGxuASntk2S"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 5.790204048156738\n",
            "Average execution time: 0.000289524678641769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1075adaa-3b0c-41f1-d8f5-4fd281f59ddd",
        "id": "w6us3NgYIJPl"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5321"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_TaxiV3, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3659560-dac2-47fb-a0c3-77067d976192",
        "id": "SKeMSdr1tk2V"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 12.984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31fbb26-e5ca-459d-e036-1eff0bd8c260",
        "id": "nkc38ERUtk2W"
      },
      "source": [
        "q_table, rewards_all, avg_time, total_time = q_learning(env_TaxiV3, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Total execution time: 8.888061285018921\n",
            "Average execution time: 0.0004444252855152218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(rewards_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72bc17a-6912-4d5c-cac7-22fc3224c9e2",
        "id": "fnxk7YsZIJ3Q"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4276.0"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env_TaxiV3, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68e5706-908b-4866-b738-d9b982a667e7",
        "id": "HhCINGyJtk2Y"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Với toy game Taxi-v3 thuật toán SARSA và Q-Learning đều có số lần chơi thành công bằng nhau và thành công trong 1000 lần chơi.\n",
        "# Tuy vậy thuật toán SARSA có hiệu quả tốt hơn về thời gian thực thi, số bước trung bình nhỏ hơn so với thuật toán Q-Learning"
      ],
      "metadata": {
        "id": "Kp_7DPoZtk2Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá chung qua 3 toy game thì thuật toán SARSA có hiệu quả tốt hơn so với thuật toán Q-Learning dù số lượng trạng thái nhiều hay ít.\n",
        "# Tuy nhiên riêng trường hợp game FrozenLake8x8-v0 không cho ra kết quả sau nhiều lần chạy (trên cả file mẫu của thầy)"
      ],
      "metadata": {
        "id": "EplrTmllcE1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}